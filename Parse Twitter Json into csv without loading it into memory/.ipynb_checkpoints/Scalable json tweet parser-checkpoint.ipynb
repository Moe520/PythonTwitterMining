{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# twitter sends its data as a json\n",
    "import json\n",
    "# we will convert the json data into a csv\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy's np.nan helps us deal with missing values (and there is a METRIC TON of missing values in tweets) \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the working directory to where the json is located. The output csv will be in this same directory\n",
    "# Windows users: remember to double-up the backward slashes!\n",
    "import os\n",
    "os.chdir(\"H:\\\\Dropbox\\\\Parsing to Prep for Tableau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the fix_encoding function from the FTFY package takes care of weird characters in tweets\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_json_file(json_file_name ,output_file_name = \"parsed_tweets.csv\", stop_at = 1000000000):\n",
    "    # Create an empty csv file that we will append to\n",
    "    # Create a header row for it\n",
    "    print(\"Initalizing Output File:  %s\"%output_file_name)\n",
    "    print(\"Generating Header Row\")\n",
    "    with open('%s'%output_file_name,'w') as f:\n",
    "        f.write('id,text,created_at,language,retweet_count,screen_name,country,user_followers_count,time_zone,user_account_location,longitude,lattitude\\n') #  Column headers and a trailing new line . MAKE SURE the /n is attached to the last field: eg. text/n\n",
    "    \n",
    "    \n",
    "    tweet_counter = 0\n",
    "    \n",
    "    for i in open(json_file_name): \n",
    "        \n",
    "        if tweet_counter > stop_at:\n",
    "            break\n",
    "            \n",
    "        \n",
    "            \n",
    "        try:\n",
    "            # Put the data from the current tweet into a list\n",
    "            # Parse the current tweet\n",
    "            current_tweet = json.loads(i)\n",
    "            \n",
    "            ##################################################################\n",
    "            \n",
    "            ## Elements that are 1 level deep ## ##\n",
    "            \n",
    "            # Get the id or insert a nan if not found\n",
    "            if 'id' in current_tweet:\n",
    "                id = current_tweet['id']\n",
    "            else:\n",
    "                id= np.nan\n",
    "             \n",
    "            # Get text or insert nan\n",
    "            \n",
    "            if 'text' in current_tweet:\n",
    "                text = current_tweet['text']\n",
    "                \n",
    "                # the fix_encoding function from the FTFY package takes care of weird characters\n",
    "                text = ftfy.fix_encoding(text)\n",
    "            else:\n",
    "                text = np.nan\n",
    "                \n",
    "                \n",
    "            # Get created_at or insert nan\n",
    "            \n",
    "            if 'created_at' in current_tweet:\n",
    "                created_at = current_tweet['created_at']\n",
    "                \n",
    "            else:\n",
    "                created_at = np.nan\n",
    "            \n",
    "            # Get language or insert nan\n",
    "           \n",
    "            if 'lang' in current_tweet:\n",
    "                language = current_tweet['lang']\n",
    "     \n",
    "            else:\n",
    "                language = np.nan     \n",
    "            \n",
    "            \n",
    "            \n",
    "            # get retweet count or insert nan\n",
    "                \n",
    "            if 'retweet_count' in current_tweet:\n",
    "                retweet_count = current_tweet['retweet_count']\n",
    "            else:\n",
    "                retweet_count = np.nan\n",
    "                \n",
    "            ## Elements that are 2 levels deep ### ###\n",
    "            \n",
    "            # For elements that are 2 layers deep use != None when searching because javascript uses None as its null operator    \n",
    "            # get screen name or insert nan\n",
    "            \n",
    "            if 'user' in current_tweet and 'screen_name' in current_tweet['user']:\n",
    "                screen_name = current_tweet['user']['screen_name']\n",
    "            else:\n",
    "                screen_name = np.nan\n",
    "                \n",
    "            # get country or insert nan\n",
    "\n",
    "            if current_tweet['place'] != None and current_tweet['place']['country'] != None:\n",
    "                country = current_tweet['place']['country']\n",
    "            else:\n",
    "                country = np.nan\n",
    "            \n",
    "            # get the author's follower count or nan\n",
    "\n",
    "            if current_tweet['user'] != None and current_tweet['user']['followers_count'] != None:\n",
    "                followers_count = current_tweet['user']['followers_count']\n",
    "            else:\n",
    "                followers_count = np.nan\n",
    "            \n",
    "            \n",
    "            # get the timezone or nan\n",
    "            if current_tweet['user'] != None and current_tweet['user']['time_zone'] != None:\n",
    "                time_zone = current_tweet['user']['time_zone']\n",
    "            else:\n",
    "                time_zone = np.nan\n",
    "                \n",
    "            # get the account location or insert nan\n",
    "            \n",
    "            if current_tweet['user'] != None and current_tweet['user']['location'] != None:\n",
    "                account_location = current_tweet['user']['location']\n",
    "                account_location = ftfy.fix_encoding(account_location)\n",
    "            else:\n",
    "                account_location = np.nan\n",
    "                \n",
    "            ###### Elements that are 3 levels deep ##################################\n",
    "            \n",
    "            if current_tweet['coordinates'] != None and current_tweet['coordinates']['coordinates'] != None and len(current_tweet['coordinates']['coordinates'])==2:\n",
    "                longitude = current_tweet['coordinates']['coordinates'][0]\n",
    "            else:\n",
    "                longitude = np.nan\n",
    "                \n",
    "            if current_tweet['coordinates'] != None and current_tweet['coordinates']['coordinates'] != None and len(current_tweet['coordinates']['coordinates'])==2:\n",
    "                lattitude = current_tweet['coordinates']['coordinates'][1]\n",
    "            else:\n",
    "                lattitude = np.nan\n",
    "        \n",
    "            ######################################################################################################\n",
    "            # Assemble the row\n",
    "            cleaned_current_tweet = [id,text,created_at,language, retweet_count, screen_name, country, followers_count,time_zone,account_location,longitude,lattitude]\n",
    "        \n",
    "            # Increment the Tweet Counter\n",
    "            tweet_counter = tweet_counter + 1\n",
    "            \n",
    "            # Give the user a progress update\n",
    "            if tweet_counter % 1000 == 0:\n",
    "                print(\" %d Tweets Parsed so far.....\" %tweet_counter)\n",
    "            \n",
    "            #append the current tweet as a row to the csv\n",
    "            with open('%s'%output_file_name,'a',newline='') as f:\n",
    "                writer=csv.writer(f)\n",
    "                writer.writerow(cleaned_current_tweet)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\" Parsing Complete:    %d Tweets Parsed \" %tweet_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalizing Output File:  my_output2.csv\n",
      "Generating Header Row\n",
      " 1000 Tweets Parsed so far.....\n",
      " 2000 Tweets Parsed so far.....\n",
      " \n",
      " Parsing Complete:    2001 Tweets Parsed \n"
     ]
    }
   ],
   "source": [
    "# This calls the parsing function. Set the parameters as follows:\n",
    "## json_file_name = the name of the json file you want to parse eg \"industry.json\"\n",
    "## output_file_name = what the csv that is output should be called eg \"my_output.csv\"\n",
    "## stop at = how many tweets to parse e.g 1000000 (if you leave it blank it will parse up to a billion tweets)\n",
    "\n",
    "stream_json_file(json_file_name=\"comparison.json\",output_file_name = \"my_output2.csv\", stop_at = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
